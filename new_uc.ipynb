{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaZeXt4msiW5gEwQewxCr1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedika1509/Data-Structure-Using-C/blob/main/new_uc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaDK61he2UsM",
        "outputId": "d4c4e787-4fa9-4e1e-e379-4cb714ebb914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Unlearnable-Clusters'...\n",
            "remote: Enumerating objects: 101, done.\u001b[K\n",
            "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 101 (delta 45), reused 74 (delta 25), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (101/101), 1.33 MiB | 20.95 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jiamingzhang94/Unlearnable-Clusters.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKb1QyWb2dMp",
        "outputId": "a459fe44-3516-455e-fd67-0273680f0f4e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tfds-nightly tensorflow matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwOIA4se29Cj",
        "outputId": "5b1e5c0a-6c87-4c8b-a614-54c327b27dcf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "_vlUwd0i3Es9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ruamel.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhcnZkyr3rog",
        "outputId": "8f828483-ba79-434f-e30a-2cf8370b29fb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel.yaml\n",
            "Successfully installed ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ruamel.yaml import YAML"
      ],
      "metadata": {
        "id": "aRDK49LU3qPC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN9plbQt4J8F",
        "outputId": "106d7e4d-f228-4e98-a013-90e7d6d899be"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install logger"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0VSdshj4SSA",
        "outputId": "4ebcf188-170d-417b-e6ab-c9b2bb514156"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting logger\n",
            "  Downloading logger-1.4.tar.gz (1.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: logger\n",
            "  Building wheel for logger (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for logger: filename=logger-1.4-py3-none-any.whl size=1758 sha256=f9119464f4d4f9c1830e9e1f75b9bcd484c0ab7e74c374111f36b121b17adf07\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/19/7b/09fc73f7503166eaf7f31b4aa0095b7f78af2ec0898e1f8312\n",
            "Successfully built logger\n",
            "Installing collected packages: logger\n",
            "Successfully installed logger-1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "DQh-X4T647O_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aVTvDbgUW-HO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "from PIL import Image\n",
        "import torch"
      ],
      "metadata": {
        "id": "sqIG9Qtb5Mfy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzoNYNhc52X1",
        "outputId": "6b719d64-2c18-4fdc-eca2-9db0a9b3f7bf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting utils\n",
            "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: utils\n",
            "  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13905 sha256=490c960bfdcb4b4b80e478184e33bc72b836b4c4a5fc7b109a632390814bfd88\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/39/f5/9d0ca31dba85773ececf0a7f5469f18810e1c8a8ed9da28ca7\n",
            "Successfully built utils\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yaml = YAML(typ='rt')"
      ],
      "metadata": {
        "id": "ZrzKZd4C6G6v"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Source and destination file paths\n",
        "source_file = '/content/Unlearnable-Clusters/logger/metricLogger.py'\n",
        "destination_directory = '/usr/local/lib/python3.10/dist-packages/logger'\n",
        "\n",
        "# Ensure destination directory exists\n",
        "os.makedirs(destination_directory, exist_ok=True)\n",
        "\n",
        "# Copy the file from source to destination\n",
        "shutil.copy(source_file, destination_directory)\n",
        "\n",
        "print(\"File copied successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dtda615KrmL",
        "outputId": "ef45f777-9718-49e5-84ca-0b9cc4fc68e4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File copied successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "\n",
        "from logger import metricLogger\n",
        "# from utils import get_surrogate, normalize_list\n",
        "\n",
        "# from dataset.dataFolder import DataFolderWithLabel, DataFolderWithOneClass\n",
        "\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth, KMeans\n",
        "import json\n"
      ],
      "metadata": {
        "id": "LIsmqqjl5eFI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "# Define the URL to download the dataset\n",
        "url = 'http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz'\n",
        "\n",
        "# Define the directory to save the downloaded file\n",
        "download_dir = '/content/data'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# Download the dataset\n",
        "print(\"Downloading Oxford-IIIT Pet dataset...\")\n",
        "urllib.request.urlretrieve(url, os.path.join(download_dir, 'images.tar.gz'))\n",
        "print(\"Download complete!\")\n",
        "\n",
        "# Extract the downloaded file\n",
        "print(\"Extracting dataset...\")\n",
        "with tarfile.open(os.path.join(download_dir, 'images.tar.gz'), 'r:gz') as tar:\n",
        "    tar.extractall(download_dir)\n",
        "print(\"Extraction complete!\")\n",
        "\n",
        "# Remove the downloaded file\n",
        "os.remove(os.path.join(download_dir, 'images.tar.gz'))\n",
        "\n",
        "print(\"Dataset downloaded and extracted successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeNpqr04Oq83",
        "outputId": "d6d2b766-cf6b-45d2-8e8d-448fc2c1f3a2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Oxford-IIIT Pet dataset...\n",
            "Download complete!\n",
            "Extracting dataset...\n",
            "Extraction complete!\n",
            "Dataset downloaded and extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# # Step 1: Zip the directory\n",
        "# directory_to_download = '/content/data'\n",
        "# shutil.make_archive(directory_to_download, 'zip', directory_to_download)\n",
        "\n",
        "# # Step 2: Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Step 3: Move the zipped directory to Google Drive\n",
        "# shutil.move(f\"{directory_to_download}.zip\", \"/content/drive/My Drive/\")\n",
        "\n",
        "# # Step 4: Download from Google Drive\n",
        "# # After running the above code, go to your Google Drive, find the zipped directory, right-click, and select \"Download\".\n"
      ],
      "metadata": {
        "id": "8XVVHP7xLsJx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define the source directory containing the downloaded images\n",
        "source_dir = '/content/data/images'\n",
        "\n",
        "# Define the destination directories for train and test data\n",
        "train_dir = '/content/data/train'\n",
        "test_dir = '/content/data/test'\n",
        "\n",
        "# Create the train and test directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Function to move a file from source to destination\n",
        "def move_file(source, destination):\n",
        "    shutil.move(source, destination)\n",
        "\n",
        "# Iterate over each image file in the source directory\n",
        "image_files = os.listdir(source_dir)\n",
        "for image_file in image_files:\n",
        "    # Get the class label from the image file name\n",
        "    label = image_file.split('_')[0]\n",
        "\n",
        "    # Create corresponding train and test directories for the label\n",
        "    train_label_dir = os.path.join(train_dir, label)\n",
        "    test_label_dir = os.path.join(test_dir, label)\n",
        "    os.makedirs(train_label_dir, exist_ok=True)\n",
        "    os.makedirs(test_label_dir, exist_ok=True)\n",
        "\n",
        "    # Determine whether the image should be used for training or testing\n",
        "    if random.random() < 0.8:  # 80% chance for training\n",
        "        destination_dir = train_label_dir\n",
        "    else:  # 20% chance for testing\n",
        "        destination_dir = test_label_dir\n",
        "\n",
        "    # Move the image file to the appropriate directory\n",
        "    move_file(os.path.join(source_dir, image_file), os.path.join(destination_dir, image_file))\n",
        "\n",
        "print(\"Dataset divided into train and test directories successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCzNK1bmK8yJ",
        "outputId": "5b0333c7-ec73-486e-e449-a4b0f961bcc8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset divided into train and test directories successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jTE2Ft8dKBLE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Define the folder containing the images\n",
        "# folder_path = '/content/data/images'\n",
        "\n",
        "# # Get the list of all files in the folder\n",
        "# files = os.listdir(folder_path)\n",
        "\n",
        "# # Filter out only JPEG images and sort them\n",
        "# jpeg_images = sorted([file for file in files if file.endswith('.jpg')])\n",
        "\n",
        "# # Get the first 5000 images\n",
        "# images_to_keep = jpeg_images[:5000]\n",
        "\n",
        "# # Delete the rest of the images and non-JPEG files\n",
        "# for file in files:\n",
        "#     if file not in images_to_keep or not file.endswith('.jpg'):\n",
        "#         os.remove(os.path.join(folder_path, file))\n",
        "\n",
        "# print(\"Operation completed successfully. Only the first 5000 JPEG images are kept in the folder.\")\n"
      ],
      "metadata": {
        "id": "DDZ4uTmj3K1d"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Source directory containing all the images\n",
        "# source_dir = \"/content/data/images\"\n",
        "\n",
        "# # Destination directories for train and test\n",
        "# train_dir = \"/content/data/train\"\n",
        "# test_dir = \"/content/data/test\"\n",
        "\n",
        "# # Create train and test directories if they don't exist\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# # Get a list of all image files in the source directory\n",
        "# image_files = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n",
        "\n",
        "# # Sort the image files to ensure consistent splitting\n",
        "# image_files.sort()\n",
        "\n",
        "# # Split the image files into train and test sets\n",
        "# train_files = image_files[:4000]\n",
        "# test_files = image_files[4000:]\n",
        "\n",
        "# # Move train files to the train directory\n",
        "# for file in train_files:\n",
        "#     src = os.path.join(source_dir, file)\n",
        "#     dst = os.path.join(train_dir, file)\n",
        "#     shutil.move(src, dst)\n",
        "\n",
        "# # Move test files to the test directory\n",
        "# for file in test_files:\n",
        "#     src = os.path.join(source_dir, file)\n",
        "#     dst = os.path.join(test_dir, file)\n",
        "#     shutil.move(src, dst)\n",
        "\n",
        "# print(\"Data split and saved successfully!\")\n"
      ],
      "metadata": {
        "id": "hkWAjlQ_3W1I"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "\n",
        "# # Specify the directory path to be deleted\n",
        "# directory_to_delete = '/content/data'\n",
        "\n",
        "# # Attempt to remove the directory and its contents\n",
        "# try:\n",
        "#     shutil.rmtree(directory_to_delete)\n",
        "#     print(f\"Directory '{directory_to_delete}' and its contents have been successfully deleted.\")\n",
        "# except OSError as e:\n",
        "#     print(f\"Error: {directory_to_delete} : {e.strerror}\")\n"
      ],
      "metadata": {
        "id": "g7YbCMIgcms_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import yaml\n",
        "\n",
        "# # Load the YAML file\n",
        "# with open('/content/Unlearnable-Clusters/config/data.yaml', 'r') as file:\n",
        "#     data = yaml.safe_load(file)\n",
        "\n",
        "# # Modify the values of 'train' and 'test'\n",
        "# data['pets']['train'] = '/content/data/train'\n",
        "# data['pets']['test'] = '/content/data/test'\n",
        "\n",
        "# data['cars']['train'] = '/content/drive/MyDrive/car196/train'\n",
        "# data['cars']['test'] = '/content/drive/MyDrive/car196/test'\n",
        "\n",
        "# # Save the updated YAML file\n",
        "# with open('updated_data.yaml', 'w') as file:\n",
        "#     yaml.dump(data, file)\n",
        "\n",
        "# print(\"YAML file updated successfully.\")\n"
      ],
      "metadata": {
        "id": "LaLZxlVaNH7h"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load YAML file\n",
        "yaml = YAML()\n",
        "with open(\"/content/Unlearnable-Clusters/config/preprocess.yaml\", 'r') as file:\n",
        "    config = yaml.load(file)\n",
        "\n",
        "# Access the configuration data\n",
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DbLfKFr3e5a",
        "outputId": "05b2e149-0d27-40af-8cac-2bc880a8020b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rename': {'data_config': '/content/Unlearnable-Clusters/config/data.yaml', 'dataset': 'pets'}, 'rename_pets': {'data_config': '/content/Unlearnable-Clusters/config/data.yaml', 'dataset': 'pets'}, 'cluster_pets_rn50_cluster10': {'data_config': '/content/Unlearnable-Clusters/config/data.yaml', 'dataset': 'pets', 'model': 'RN50', 'checkpoint': '/content/Unlearnable-Clusters/checkpoints/RN50_imagenet.pth', 'normalize': 'imagenet', 'num_classes': 1000, 'num_clusters': 10, 'output_dir': '/content/Unlearnable-Clusters/output/cluster'}, 'cluster_pets_cliprn50_cluster10': {'data_config': '/content/Unlearnable-Clusters/config/data.yaml', 'dataset': 'pets', 'model': 'CLIPRN50', 'checkpoint': 'checkpoints/RN50_clip.pth', 'normalize': 'clip', 'num_classes': 1000, 'num_clusters': 10, 'output_dir': '/content/Unlearnable-Clusters/output/cluster'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load YAML file\n",
        "yaml = YAML()\n",
        "yaml_file_path = \"/content/Unlearnable-Clusters/config/preprocess.yaml\"  # Replace with the actual path to your YAML file\n",
        "with open(yaml_file_path, 'r') as file:\n",
        "    config = yaml.load(file)\n",
        "\n",
        "# Check if the function 'rename' exists in the YAML file\n",
        "if 'rename' in config:\n",
        "    # Execute the 'rename' function\n",
        "    rename_config = config['rename']\n",
        "    # Implement the logic to execute the 'rename' function here\n",
        "\n",
        "    print(\"Function 'rename' executed successfully.\")\n",
        "else:\n",
        "    print(\"Function 'rename' not found in the YAML file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4na4gD832hD",
        "outputId": "9de01f2a-ce27-415b-e2e5-d2557ab9e7c0"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'rename' executed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyYAML"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgFsTKooMmVn",
        "outputId": "e5571780-a7cb-4b08-ad03-6e9a665e3f5f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (6.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "with open(\"/content/Unlearnable-Clusters/config/preprocess.yaml\", 'r') as file:\n",
        "    config = yaml.load(file, Loader=yaml.Loader)"
      ],
      "metadata": {
        "id": "QsQjBsE4MMjK"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ruamel.yaml import YAML\n",
        "\n",
        "# Assuming args.config\n"
      ],
      "metadata": {
        "id": "HTifqdRTPwhH"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    #     # Load YAML file using yaml.safe_load()\n",
        "    # with open(args.config, 'r') as file:\n",
        "    #     config = yaml.safe_load(file)[args.function]\n",
        "\n",
        "    # # Load data config\n",
        "    # with open(config['data_config'], 'r') as file:\n",
        "    #     data_config = yaml.safe_load(file)[config['dataset']]\n",
        "\n",
        "#     from ruamel.yaml import YAML\n",
        "\n",
        "# # Create an instance of the YAML class\n",
        "# yaml = YAML(typ='rt')\n",
        "\n",
        "# # Load YAML data from file\n",
        "# with open(config['data_config'], 'r') as file:\n",
        "#     data_config = yaml.load(file)[config['dataset']]\n"
      ],
      "metadata": {
        "id": "RRLxMEjiQpr4"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyYAML\n",
        "import yaml\n",
        "stream = open('/content/Unlearnable-Clusters/config/preprocess.yaml', 'r')\n",
        "data = yaml.full_load(stream)\n",
        "stream.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXyPKK6TUS3o",
        "outputId": "12169a1d-6cd1-4d28-b424-880b7e80736c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (6.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5LSwifsWy2I",
        "outputId": "3c38742a-778c-41b2-f407-571f275f6b4e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dataset in /usr/local/lib/python3.10/dist-packages (1.6.2)\n",
            "Requirement already satisfied: sqlalchemy<2.0.0,>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from dataset) (1.4.52)\n",
            "Requirement already satisfied: alembic>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from dataset) (1.13.1)\n",
            "Requirement already satisfied: banal>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from dataset) (1.0.6)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (1.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (4.10.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=0.6.2->dataset) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/Unlearnable-Clusters/dataset/dataCluster.py"
      ],
      "metadata": {
        "id": "eOiFRItJcrvq"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/Unlearnable-Clusters/dataset/dataFolder.py"
      ],
      "metadata": {
        "id": "NJTpwbKZc02G"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/Unlearnable-Clusters/checkpoints/__init__.py"
      ],
      "metadata": {
        "id": "M4T2vOFYc6E1"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ruamel.yaml import YAML"
      ],
      "metadata": {
        "id": "r9j7v05HlCFG"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "def is_image_file(filename):\n",
        "    IMG_EXTENSIONS = [\n",
        "        '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
        "        '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.tiff'\n",
        "    ]\n",
        "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
        "\n",
        "\n",
        "class DataFolderWithLabel(Dataset):\n",
        "    def __init__(self, root, pred_idx, transform=None):\n",
        "        self.labels = []\n",
        "        self.images = []\n",
        "        self.transform = transform\n",
        "\n",
        "        for class_name in sorted(os.listdir(root)):\n",
        "            label = int(class_name)\n",
        "            for file_name in sorted(os.listdir(os.path.join(root, class_name))):\n",
        "                if not is_image_file(file_name):\n",
        "                    continue\n",
        "                self.images.append(os.path.join(root, class_name, file_name))\n",
        "                self.labels.append(label)\n",
        "\n",
        "        if pred_idx is None:\n",
        "            self.pred_idx = self.labels\n",
        "        else:\n",
        "            self.pred_idx = pred_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label, self.pred_idx[idx]\n",
        "\n",
        "\n",
        "class DataFolderWithClassNoise(Dataset):\n",
        "    def __init__(self, root, pred_idx, noise=None, resize_type='img'):\n",
        "        self.labels = []\n",
        "        self.images = []\n",
        "        self.resize_type = resize_type\n",
        "        if self.resize_type == 'img':\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "            ])\n",
        "\n",
        "        for class_name in sorted(os.listdir(root)):\n",
        "            label = int(class_name)\n",
        "            for file_name in sorted(os.listdir(os.path.join(root, class_name))):\n",
        "                if not is_image_file(file_name):\n",
        "                    continue\n",
        "                self.images.append(os.path.join(root, class_name, file_name))\n",
        "                self.labels.append(label)\n",
        "\n",
        "        if noise is None:\n",
        "            self.noise = torch.zeros((1, 3, 112, 112))\n",
        "            self.noise.uniform_(0, 1)\n",
        "            self.noise = self.noise.repeat(self.num_classes, 1, 1, 1)\n",
        "        else:\n",
        "            self.noise = noise\n",
        "\n",
        "        self.pred_idx = pred_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.resize_type == 'img':\n",
        "            image = torch.clamp(image + self.noise[self.pred_idx[idx]], 0, 1)\n",
        "        else:\n",
        "            img_size = image.size()\n",
        "            noise = torch.nn.functional.interpolate(self.noise[self.pred_idx[idx]].unsqueeze(0), (img_size[1], img_size[2]))\n",
        "            image = torch.clamp(image + noise, 0, 1)\n",
        "        return image, label, self.pred_idx[idx]\n",
        "\n",
        "\n",
        "class DataFolderWithOneClass(Dataset):\n",
        "    def __init__(self, root, cluster_idx, pred_idx, transform=None, offset=0):\n",
        "        self.images = []\n",
        "        self.label = []\n",
        "        self.transform = transform\n",
        "\n",
        "        for class_name in sorted(os.listdir(root)):\n",
        "            label = int(class_name) - offset\n",
        "            for file_name in sorted(os.listdir(os.path.join(root, class_name))):\n",
        "                if not is_image_file(file_name):\n",
        "                    continue\n",
        "                self.images.append(os.path.join(root, class_name, file_name))\n",
        "                self.label.append(label)\n",
        "\n",
        "        self.pred_idx = pred_idx\n",
        "        self.cluster_idx = cluster_idx\n",
        "\n",
        "        self.images = [self.images[i] for i, eq in enumerate(pred_idx == cluster_idx) if eq]\n",
        "        self.label = [self.label[i] for i, eq in enumerate(pred_idx == cluster_idx) if eq]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.label[idx], self.cluster_idx\n"
      ],
      "metadata": {
        "id": "HfwF4KATmBm7"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/Unlearnable-Clusters/preprocess.py --config /content/Unlearnable-Clusters/config/preprocess.yaml -f cluster_pets_rn50_cluster10\n",
        "# the cluster file will be stored in {output_dir}/{dataset}_{surrogate_model}_cluster{num_clusters}.pth\n",
        "# e.g., output/cluster/pets_rn50_cluster10.pth"
      ],
      "metadata": {
        "id": "kGaEn-fuc_Hb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06720c2b-7ad4-4e72-a323-99305c942ca6"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Unlearnable-Clusters/preprocess.py\", line 157, in <module>\n",
            "    data_cluster(args, config)\n",
            "  File \"/content/Unlearnable-Clusters/preprocess.py\", line 71, in data_cluster\n",
            "    sd = torch.load(config['checkpoint'], map_location='cpu')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 986, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 435, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 416, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/Unlearnable-Clusters/checkpoints/RN50_imagenet.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lmNdXfmuVWCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLUkr7ciY7pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ruamel.yaml import YAML\n",
        "import argparse\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from logger import metricLogger\n",
        "# from utils import get_surrogate, normalize_list\n",
        "# from dataset.dataFolder import DataFolderWithLabel, DataFolderWithOneClass\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth, KMeans\n",
        "import json\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "# !pip install PyYAML\n",
        "import yaml\n",
        "\n",
        "def dataset_median_embedding(args, config):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    normalize = normalize_list[args.normalize]\n",
        "\n",
        "    num_classes = len(os.listdir(config[args.dataset]['test']['path']))\n",
        "    net = get_model(args.model, num_classes).eval().to(args.device)\n",
        "    sd = torch.load(args.checkpoint, map_location='cpu')\n",
        "    sd = sd.get('state_dict', sd)\n",
        "    net.load_state_dict(sd)\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        train_dataset = DataFolderWithOneClass(config[args.dataset]['train']['path'], i, transform)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, num_workers=8)\n",
        "\n",
        "        features = []\n",
        "        def hook(layer, inp, out):\n",
        "            features.append(inp[0].cpu())\n",
        "        net.fc.register_forward_hook(hook)\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(args.device), labels.to(args.device)\n",
        "            with torch.no_grad():\n",
        "                net(normalize(images))\n",
        "\n",
        "        features = torch.cat(features, dim=0)\n",
        "        dis = squareform(pdist(features.numpy())).sum(axis=1)\n",
        "        dis = torch.tensor(dis)\n",
        "        idx = torch.argmin(dis)\n",
        "        result.append(features[idx])\n",
        "\n",
        "    torch.save(result, os.path.join(args.output_dir, f'{args.dataset}_{args.model}_median_embed.pth'))\n",
        "\n",
        "\n",
        "def data_cluster(args, config):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    normalize = normalize_list[config['normalize']]\n",
        "\n",
        "    train_dataset = DataFolderWithLabel(config['dataset']['config']['train'], transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, num_workers=4)\n",
        "\n",
        "    sd = torch.load(config['checkpoint'], map_location='cpu')\n",
        "    sd = sd.get('state_dict', sd)\n",
        "    net = get_model(config['model'], config['num_classes']).eval().to(args.device)\n",
        "    net.load_state_dict(sd)\n",
        "\n",
        "    features = []\n",
        "\n",
        "    def hook(layer, inp, out):\n",
        "        features.append(inp[0].cpu())\n",
        "\n",
        "    net.fc.register_forward_hook(hook)\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(args.device), labels.to(args.device)\n",
        "        with torch.no_grad():\n",
        "            net(normalize(images))\n",
        "\n",
        "    features = torch.cat(features, dim=0)\n",
        "    classifier = KMeans(n_clusters=config['num_clusters'])\n",
        "    pred_idx = classifier.fit_predict(features.numpy())\n",
        "\n",
        "    test_dataset = DataFolderWithLabel(config['dataset']['config']['test'], transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=256, num_workers=8)\n",
        "\n",
        "    features = []\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(args.device), labels.to(args.device)\n",
        "        with torch.no_grad():\n",
        "            net(normalize(images))\n",
        "\n",
        "    features = torch.cat(features, dim=0)\n",
        "    pred_idx_test = classifier.predict(features.numpy())\n",
        "\n",
        "    result = {'pred_idx': torch.tensor(pred_idx), 'pred_idx_test': pred_idx_test, 'centers': torch.tensor(classifier.cluster_centers_)}\n",
        "    num_clusters = result['centers'].shape[0]\n",
        "    torch.save(result, os.path.join(config['output_dir'], f\"{config['dataset']['name']}_{config['model'].lower()}_cluster{num_clusters}.pth\"))\n",
        "\n",
        "\n",
        "def datafolder_rename(args, config):\n",
        "    train_dir = config['dataset']['config']['train']\n",
        "    test_dir = config['dataset']['config']['test']\n",
        "\n",
        "    class_list = sorted(os.listdir(train_dir))\n",
        "    new_class = list(map(str, range(len(class_list))))\n",
        "\n",
        "    name_mapping = {}\n",
        "    name_mapping['map'] = {k: v for k, v in zip(class_list, new_class)}\n",
        "    name_mapping['inv'] = {k: v for k, v in zip(new_class, class_list)}\n",
        "\n",
        "    json.dump(name_mapping, open(os.path.join(train_dir, '..', 'name_mapping.json'), 'w+'))\n",
        "\n",
        "    os.rename(train_dir, train_dir+'_raw')\n",
        "    os.rename(test_dir, test_dir+'_raw')\n",
        "    os.mkdir(train_dir)\n",
        "    os.mkdir(test_dir)\n",
        "    for i in range(len(class_list)):\n",
        "        shutil.copytree(os.path.join(train_dir+'_raw', class_list[i]), os.path.join(train_dir, new_class[i]))\n",
        "        shutil.copytree(os.path.join(test_dir+'_raw', class_list[i]), os.path.join(test_dir, new_class[i]))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--device', type=int, default=0)\n",
        "    parser.add_argument('--config', type=str)\n",
        "\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "\n",
        "    parser.add_argument('--function', '-f', type=str)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "\n",
        "    # Load YAML file using yaml.load()\n",
        "    with open(args.config, 'r') as file:\n",
        "        config = yaml.full_load(file)\n",
        "\n",
        "    # Access the required data\n",
        "    config = config[args.function]\n",
        "\n",
        "    if args.function.startswith('median'):\n",
        "        Path(config['output_dir']).mkdir(parents=True, exist_ok=True)\n",
        "        dataset_median_embedding(args, config)\n",
        "    elif args.function.startswith('cluster'):\n",
        "        Path(config['output_dir']).mkdir(parents=True, exist_ok=True)\n",
        "        data_cluster(args, config)\n",
        "    elif args.function.startswith('rename'):\n",
        "        datafolder_rename(args, config)\n"
      ],
      "metadata": {
        "id": "qrTlO59qWbI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataFolderWithLabel(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.labels = []\n",
        "        self.images = []\n",
        "        self.transform = transform\n",
        "\n",
        "        # Dictionary to map directory names to integer labels\n",
        "        class_mapping = {\n",
        "    'Abyssinian': 0,\n",
        "    'Bengal': 1,\n",
        "    'Birman': 2,\n",
        "    'Bombay': 3,\n",
        "    'British_Shorthair': 4,\n",
        "    'Egyptian_Mau': 5,\n",
        "    'Maine_Coon': 6,\n",
        "    'Persian': 7,\n",
        "    'Ragdoll': 8,\n",
        "    'Russian_Blue': 9,\n",
        "    'Siamese': 10,\n",
        "    'Sphynx': 11\n",
        "}\n",
        "\n",
        "\n",
        "        for class_name in sorted(os.listdir(root)):\n",
        "            label = class_mapping.get(class_name)\n",
        "            if label is not None:\n",
        "                for file_name in sorted(os.listdir(os.path.join(root, class_name))):\n",
        "                    if not is_image_file(file_name):\n",
        "                        continue\n",
        "                    self.images.append(os.path.join(root, class_name, file_name))\n",
        "                    self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "dv6zbOKnaGK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DataFolderWithLabel('/content/data/train')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCadOOZ_aovI",
        "outputId": "65ec47d8-bce5-447f-8f80-3dbc222f4120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.DataFolderWithLabel at 0x7f5407615d20>"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ruamel.yaml import YAML  # Import the YAML module\n",
        "\n",
        "# # Create a YAML instance\n",
        "# yaml = YAML(typ='rt')\n",
        "\n",
        "# # Load the YAML file\n",
        "with open('/content/Unlearnable-Clusters/config/preprocess.yaml', 'r') as file:\n",
        "    config = yaml.load(file)\n",
        "\n",
        "# # Access the required data\n",
        "config = config['cluster_pets_rn50_cluster10']\n",
        "\n",
        "# Execute preprocessing script with the specified YAML configuration\n",
        "!python /content/Unlearnable-Clusters/preprocess.py --config /content/Unlearnable-Clusters/config/preprocess.yaml -f cluster_pets_rn50_cluster10\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8d7evc73_c8",
        "outputId": "b3a71371-97f8-41d4-9616-78f9872c7e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/Unlearnable-Clusters/preprocess.py\", line 9, in <module>\n",
            "    from utils import get_surrogate, normalize_list\n",
            "  File \"/content/Unlearnable-Clusters/utils/__init__.py\", line 1, in <module>\n",
            "    from .util import adjust_learning_rate, get_surrogate, get_target, normalize_list\n",
            "  File \"/content/Unlearnable-Clusters/utils/util.py\", line 1, in <module>\n",
            "    from models import *\n",
            "ModuleNotFoundError: No module named 'models'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from ruamel.yaml import YAML\n",
        "# import argparse\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import shutil\n",
        "# from pathlib import Path\n",
        "# from logger import MetricLogger\n",
        "# from utils import get_surrogate, normalize_list\n",
        "# from dataset.dataFolder import DataFolderWithLabel, DataFolderWithOneClass\n",
        "# from scipy.spatial.distance import pdist, squareform\n",
        "# from sklearn.cluster import MeanShift, estimate_bandwidth, KMeans\n",
        "# import json\n",
        "# from torchvision import transforms\n",
        "# from torch.utils.data import DataLoader\n",
        "# # !pip install PyYAML\n",
        "# import yaml\n",
        "\n",
        "# def dataset_median_embedding(args, config):\n",
        "#     transform = transforms.Compose([\n",
        "#         transforms.Resize(256),\n",
        "#         transforms.CenterCrop(224),\n",
        "#         transforms.ToTensor(),\n",
        "#     ])\n",
        "\n",
        "#     normalize = normalize_list[args.normalize]\n",
        "\n",
        "#     num_classes = len(os.listdir(config[args.dataset]['test']['path']))\n",
        "#     net = get_model(args.model, num_classes).eval().to(args.device)\n",
        "#     sd = torch.load(args.checkpoint, map_location='cpu')\n",
        "#     sd = sd.get('state_dict', sd)\n",
        "#     net.load_state_dict(sd)\n",
        "\n",
        "#     result = []\n",
        "\n",
        "#     for i in range(num_classes):\n",
        "#         train_dataset = DataFolderWithOneClass(config[args.dataset]['train']['path'], i, transform)\n",
        "#         train_loader = DataLoader(train_dataset, batch_size=64, num_workers=8)\n",
        "\n",
        "#         features = []\n",
        "#         def hook(layer, inp, out):\n",
        "#             features.append(inp[0].cpu())\n",
        "#         net.fc.register_forward_hook(hook)\n",
        "\n",
        "#         for images, labels in train_loader:\n",
        "#             images, labels = images.to(args.device), labels.to(args.device)\n",
        "#             with torch.no_grad():\n",
        "#                 net(normalize(images))\n",
        "\n",
        "#         features = torch.cat(features, dim=0)\n",
        "#         dis = squareform(pdist(features.numpy())).sum(axis=1)\n",
        "#         dis = torch.tensor(dis)\n",
        "#         idx = torch.argmin(dis)\n",
        "#         result.append(features[idx])\n",
        "\n",
        "#     torch.save(result, os.path.join(args.output_dir, f'{args.dataset}_{args.model}_median_embed.pth'))\n",
        "\n",
        "\n",
        "# def data_cluster(args, config):\n",
        "#     transform = transforms.Compose([\n",
        "#         transforms.Resize(256),\n",
        "#         transforms.CenterCrop(224),\n",
        "#         transforms.ToTensor(),\n",
        "#     ])\n",
        "\n",
        "#     normalize = normalize_list[config['normalize']]\n",
        "\n",
        "#     train_dataset = DataFolderWithLabel(config['dataset']['config']['train'], transform)\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=64, num_workers=4)\n",
        "\n",
        "#     sd = torch.load(config['checkpoint'], map_location='cpu')\n",
        "#     sd = sd.get('state_dict', sd)\n",
        "#     net = get_model(config['model'], config['num_classes']).eval().to(args.device)\n",
        "#     net.load_state_dict(sd)\n",
        "\n",
        "#     features = []\n",
        "\n",
        "#     def hook(layer, inp, out):\n",
        "#         features.append(inp[0].cpu())\n",
        "\n",
        "#     net.fc.register_forward_hook(hook)\n",
        "\n",
        "#     for images, labels in train_loader:\n",
        "#         images, labels = images.to(args.device), labels.to(args.device)\n",
        "#         with torch.no_grad():\n",
        "#             net(normalize(images))\n",
        "\n",
        "#     features = torch.cat(features, dim=0)\n",
        "#     classifier = KMeans(n_clusters=config['num_clusters'])\n",
        "#     pred_idx = classifier.fit_predict(features.numpy())\n",
        "\n",
        "#     test_dataset = DataFolderWithLabel(config['dataset']['config']['test'], transform)\n",
        "#     test_loader = DataLoader(test_dataset, batch_size=256, num_workers=8)\n",
        "\n",
        "#     features = []\n",
        "#     for images, labels in test_loader:\n",
        "#         images, labels = images.to(args.device), labels.to(args.device)\n",
        "#         with torch.no_grad():\n",
        "#             net(normalize(images))\n",
        "\n",
        "#     features = torch.cat(features, dim=0)\n",
        "#     pred_idx_test = classifier.predict(features.numpy())\n",
        "\n",
        "#     result = {'pred_idx': torch.tensor(pred_idx), 'pred_idx_test': pred_idx_test, 'centers': torch.tensor(classifier.cluster_centers_)}\n",
        "#     num_clusters = result['centers'].shape[0]\n",
        "#     torch.save(result, os.path.join(config['output_dir'], f\"{config['dataset']['name']}_{config['model'].lower()}_cluster{num_clusters}.pth\"))\n",
        "\n",
        "\n",
        "# def datafolder_rename(args, config):\n",
        "#     train_dir = config['dataset']['config']['train']\n",
        "#     test_dir = config['dataset']['config']['test']\n",
        "\n",
        "#     class_list = sorted(os.listdir(train_dir))\n",
        "#     new_class = list(map(str, range(len(class_list))))\n",
        "\n",
        "#     name_mapping = {}\n",
        "#     name_mapping['map'] = {k: v for k, v in zip(class_list, new_class)}\n",
        "#     name_mapping['inv'] = {k: v for k, v in zip(new_class, class_list)}\n",
        "\n",
        "#     json.dump(name_mapping, open(os.path.join(train_dir, '..', 'name_mapping.json'), 'w+'))\n",
        "\n",
        "#     os.rename(train_dir, train_dir+'_raw')\n",
        "#     os.rename(test_dir, test_dir+'_raw')\n",
        "#     os.mkdir(train_dir)\n",
        "#     os.mkdir(test_dir)\n",
        "#     for i in range(len(class_list)):\n",
        "#         shutil.copytree(os.path.join(train_dir+'_raw', class_list[i]), os.path.join(train_dir, new_class[i]))\n",
        "#         shutil.copytree(os.path.join(test_dir+'_raw', class_list[i]), os.path.join(test_dir, new_class[i]))\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument('--device', type=int, default=0)\n",
        "#     parser.add_argument('--config', type=str)\n",
        "\n",
        "#     parser.add_argument('--seed', type=int, default=42)\n",
        "\n",
        "#     parser.add_argument('--function', '-f', type=str)\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     torch.manual_seed(args.seed)\n",
        "#     torch.cuda.manual_seed(args.seed)\n",
        "#     np.random.seed(args.seed)\n",
        "\n",
        "#     # Load YAML file using yaml.load()\n",
        "#     with open(args.config, 'r') as file:\n",
        "#         config = yaml.full_load(file)\n",
        "\n",
        "#     # Access the required data\n",
        "#     config = config[args.function]\n",
        "\n",
        "#     if args.function.startswith('median'):\n",
        "#         Path(config['output_dir']).mkdir(parents=True, exist_ok=True)\n",
        "#         dataset_median_embedding(args, config)\n",
        "#     elif args.function.startswith('cluster'):\n",
        "#         Path(config['output_dir']).mkdir(parents=True, exist_ok=True)\n",
        "#         data_cluster(args, config)\n",
        "#     elif args.function.startswith('rename'):\n",
        "#         datafolder_rename(args, config)\n"
      ],
      "metadata": {
        "id": "4Bo2drUR4xBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ZzhPb3D5GOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m5Ct1l6N5TUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "3jOe17Id8Apy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wuvKCS9H861E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TfBaRfHIga3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "glqJ_kBM5iFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9p0JJfH36dWN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}